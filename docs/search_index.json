[["index.html", "Data Analysis: Regression Modelling Introduction", " Data Analysis: Regression Modelling Introduction Now that you are familiar with RMarkdown, you are encouraged to collate your work in this lab in a RMarkdown file. Complete the lab from this week and subsequent weeks in a .Rmd file. Now that we have covered visualising and manipulating data in R, we can now proceed to modelling data. The key idea behind modelling data is to infer the relationship between an: outcome (or response) variable \\(y\\) and an explanatory (or predictor) variable(s) \\(x\\), which can also be referred to as an independent variable(s) or covariate(s). Modelling can be used for two purposes: Explanation: For describing the relationship between an outcome variable \\(y\\) and an explanatory variable \\(x\\), and determining the potential significance of such relationships using quantifiable measures. Prediction: for predicting the outcome variable \\(y\\) given information from one or more explanatory variables. There are many different modelling techniques. However, we will begin with one of the easier to understand and commonly-used approaches, linear regression. In particular, we will start by looking at simple linear regression (SLR), where we only have one explanatory variable. We will then extend these models to allow for more than one explanatory variable using multiple linear regression (MLR). Note: Additional information and examples can be found in Chapter 5 and Chapter 6 in Statistical Inference via Data Science. "],["simple-linear-regression.html", "Simple linear regression", " Simple linear regression For a response variable \\(y\\) and an explanatory variable \\(x\\), the data can be expressed as: \\[(y_i, x_i), ~~~~ i = 1,\\ldots,n.\\] That is, we have \\(n\\) observations of \\(y\\) and \\(x\\). A statistical model is a mathematical statement describing the variability in a random variable \\(y\\), which includes any relationship with the explanatory variable \\(x\\). The inclusion of random (unpredictable) components \\(\\epsilon\\), makes the model statistical, rather than deterministic. A simple linear regression model involves, as the name suggests, fitting a linear regression line to the data. Hence, a simple linear regression model can be written as follows: \\[y_i = \\alpha + \\beta x_i + \\epsilon_i, ~~~~ \\epsilon_i \\sim N(0, \\sigma^2),\\] where \\(y_i\\) is the \\(i^{th}\\) observation of the response variable; \\(\\alpha\\) is the intercept of the regression line; \\(\\beta\\) is the slope of the regression line; \\(x_i\\) is the \\(i^{th}\\) observation of the explanatory variable; and \\(\\epsilon_i\\) is the \\(i^{th}\\) random component. The random components, \\(\\epsilon_i\\), are normally distributed with mean zero and constant variance \\(\\sigma^2\\), such that we are essentially adding random white noise to the deterministic part of the model (\\(\\alpha + \\beta x_i\\)). Thus, the full probability model for \\(y_i\\) given \\(x_i\\) (\\(y_i | x_i\\)) can be written as \\[y_i | x_i \\sim N(\\alpha + \\beta x_i, \\sigma^2).\\] Hence, the mean comes from the deterministic part of the model, while the variance comes from the random part. We will focus on the topic of choosing between different models in a future lab but for now we recall that one way to decide which terms should be included in the model is to consider the p-value associated with each estimated parameter (i.e. in this example \\(\\hat\\alpha\\) and \\(\\hat\\beta\\)). The convention is to choose a significance level of 5% and then decide to include terms in the model if the p-value of the associated parameter estimate is less than 0.05 (i.e. reject the null hypothesis that the parameter is zero). This approach is used with both SLR and MLR models. "],["simple-linear-regression-with-one-numerical-explanatory-variable.html", "Simple linear regression with one numerical explanatory variable Exploratory data analysis Correlation Formal analysis Assessing model fit", " Simple linear regression with one numerical explanatory variable Create a .Rmd file to load the following packages into R: library(ggplot2) library(dplyr) library(moderndive) library(gapminder) library(skimr) Student feedback in higher education is extremely important when it comes to the evaluation of teaching techniques, materials, and improvements in teaching methods and technologies. However, there have been studies into potential bias factors when feedback is provided, such as the physical appearance of the teacher; see Economics of Education Review for details. Here, we shall look at a study from student evaluations of \\(n=463\\) professors from The University of Texas at Austin. In particular, we will examine the evaluation scores of the instructors based purely on one numerical variable: their beauty score. Therefore, our simple linear regression model will consist of: the numerical outcome variable teaching score (\\(y\\)); and the numerical explanatory variable beauty score (\\(x\\)). Exploratory data analysis Before you ever do any statistical modelling of data, you should always perform an exploratory data analysis of the data. Performing an exploratory data analysis can give us an idea of the distribution of the data, and whether it contains any strange values, such as outliers or missing values. However, more importantly, it is used to inform which statistical model we should fit to the data. An exploratory data analysis may involve: Looking at the raw values of the data, either by looking at the spreadsheet directly, or using R. By computing various summary statistics, such as the five-number summary, means, and standard deviations. Plotting the data using various data visualisation techniques. Let's examine the data evals. We can look at the raw values from evals using the RStudio pop-up spreadsheet viewer using: View(evals) Task: Use other functions you have learned to have a peek at the evals data set. Hint You may want to use the glimpse and head functions. Solution head(evals) # A tibble: 6 x 14 ID prof_ID score age bty_avg gender ethni~1 langu~2 rank pic_o~3 pic_c~4 &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 1 1 4.7 36 5 female minori~ english tenu~ not fo~ color 2 2 1 4.1 36 5 female minori~ english tenu~ not fo~ color 3 3 1 3.9 36 5 female minori~ english tenu~ not fo~ color 4 4 1 4.8 36 5 female minori~ english tenu~ not fo~ color 5 5 2 4.6 59 3 male not mi~ english tenu~ not fo~ color 6 6 2 4.3 59 3 male not mi~ english tenu~ not fo~ color # ... with 3 more variables: cls_did_eval &lt;int&gt;, cls_students &lt;int&gt;, # cls_level &lt;fct&gt;, and abbreviated variable names 1: ethnicity, 2: language, # 3: pic_outfit, 4: pic_color # i Use `colnames()` to see all variable names glimpse(evals) Rows: 463 Columns: 14 $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~ $ prof_ID &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, ~ $ score &lt;dbl&gt; 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.~ $ age &lt;int&gt; 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 4~ $ bty_avg &lt;dbl&gt; 5.000, 5.000, 5.000, 5.000, 3.000, 3.000, 3.000, 3.333, 3~ $ gender &lt;fct&gt; female, female, female, female, male, male, male, male, m~ $ ethnicity &lt;fct&gt; minority, minority, minority, minority, not minority, not~ $ language &lt;fct&gt; english, english, english, english, english, english, eng~ $ rank &lt;fct&gt; tenure track, tenure track, tenure track, tenure track, t~ $ pic_outfit &lt;fct&gt; not formal, not formal, not formal, not formal, not forma~ $ pic_color &lt;fct&gt; color, color, color, color, color, color, color, color, c~ $ cls_did_eval &lt;int&gt; 24, 86, 76, 77, 17, 35, 39, 55, 111, 40, 24, 24, 17, 14, ~ $ cls_students &lt;int&gt; 43, 125, 125, 123, 20, 40, 44, 55, 195, 46, 27, 25, 20, 2~ $ cls_level &lt;fct&gt; upper, upper, upper, upper, upper, upper, upper, upper, u~ How many observations are in the evals data set? 512 18 13 463 At the moment we are only really interested in the instructors teaching (score) and beauty (bty_avg) scores, and so we can look at a subset of the data as follows: evals.scores &lt;- evals %&gt;% select(score, bty_avg) Task: Replace the select function with the sample_n function to look at a random subset of 10 instructors. Solution evals %&gt;% sample_n(10) # A tibble: 10 x 14 ID prof_ID score age bty_avg gender ethnicity language rank pic_o~1 &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 120 20 3.4 57 4.33 female not minority english teac~ not fo~ 2 335 68 2.4 60 1.67 male not minority english tenu~ not fo~ 3 299 58 4.7 43 3.33 female not minority english tenu~ not fo~ 4 152 27 3.6 52 4.83 male minority non-engl~ tenu~ formal 5 420 85 5 58 7.83 male not minority english teac~ not fo~ 6 318 64 4 52 6.5 female not minority english tenu~ not fo~ 7 117 20 3.3 57 4.33 female not minority english teac~ not fo~ 8 255 49 3.5 52 3.17 male not minority english tenu~ not fo~ 9 421 85 5 58 7.83 male not minority english teac~ not fo~ 10 424 85 5 58 7.83 male not minority english teac~ not fo~ # ... with 4 more variables: pic_color &lt;fct&gt;, cls_did_eval &lt;int&gt;, # cls_students &lt;int&gt;, cls_level &lt;fct&gt;, and abbreviated variable name # 1: pic_outfit # i Use `colnames()` to see all variable names The outcome variable score is a numerical average of the average teaching score based on students' evaluations between 1 and 5. The explanatory variable bty_avg is the numerical variable of the average beauty score from a panel of six students' scores between 1 and 10. As both variables are numerical, we can compute summary statistics for them using the skim function from the skimr package as follows: evals.scores %&gt;% skim() Table 1: Data summary Name Piped data Number of rows 463 Number of columns 2 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00  bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17  This provides us with the following information: missing: the number of missing values. complete: the number of non-missing values. n: the total number of observations. mean: the mean or average. sd: the standard deviation. p0: the \\(0^{th}\\) percentile: the value at which 0% of values are smaller than it (i.e. the minimum). p25: the \\(25^{th}\\) percentile: the value at which 25% of values are smaller than it (i.e. the 1st quartile). p50: the \\(50^{th}\\) percentile: the value at which 50% of values are smaller than it (i.e. the median). p75: the \\(75^{th}\\) percentile: the value at which 75% of values are smaller than it (i.e. the 3rd quartile). p100: the \\(100^{th}\\) percentile: the value at which 100% of values are smaller than it (i.e. the maximum). hist: provides a snapshot of a histogram of the variable. These summary statistics give us an idea of how both variables are distributed. For example, the mean teaching score (score) is 4.17 out 5, while the mean beauty score (bty_avg) is 4.42 out of 10. Also, the middle 50% of the data for score lies between 3.8 and 4.6, while the middle 50% of bty_avg lies between 3.17 and 5.5. What is the minimum value of teaching score (score)? 2.3 1.7 1.6 2.5 What is the maximum value of beauty score (bty_avg)? 8.4 8.2 5.2 5.0 Correlation The above summary statistics provide information about each variable separately. However, we are interested in a potential relationship between the two variables and as such it would be of interest to evaluate some statistic that considers both variables simultaneously. One such statistic is the correlation, which ranges between -1 and 1 and describes the strength of the linear relationship between two numerical variables, such that -1 indicates a perfect negative relationship. That is, as the values of one variable increase, the values of the other decrease. 0 indicates no relationship. The values of both variables increase/decrease independently of one another. 1 indicates a perfect positive relationship. That is, the values of both variables increase simultaneously. The plot below displays scatterplots for hypothetical numerical variables \\(x\\) and \\(y\\) simulated to have different levels of correlation. Figure 1: Differing levels of correlation between variables. Which plots (a)-(e) display positive correlation? (d) and (e) (c) and (e) (c) and (d) (a) and (b) Which plots (a)-(e) display negative correlation? (a) and (c) (d) and (e) (b) and (c) (a) and (b) The correlation coefficient can be computed in R using the get_correlation function from the moderndive package. The function requires two numerical variables separated by ~ (or 'tilde'), much like the formula syntax, such that the outcome variable score is put on the left-hand-side of the formula, and the explanatory variable bty_avg is placed on the right-hand-side of the formula: evals.scores %&gt;% get_correlation(formula = score ~ bty_avg) # A tibble: 1 x 1 cor &lt;dbl&gt; 1 0.187 Here, we are given a correlation coefficient of 0.187 for the relationship between teaching (score) and beauty (bty_avg) scores. This suggests a rather weakly positive linear relationship between the two variables. There is some subjective interpretation surrounding correlation coefficients not very close to -1, 0, 1. The table below provides a rough guide as to the verbal interpretation of a correlation coefficient. Correlation coefficient Verbal interpretation 0.90 to 1.00 (-0.90 to -1.00) Very strong positive (negative) correlation 0.70 to 0.90 (-0.70 to -0.90) Strong positive (negative) correlation 0.50 to 0.70 (-0.50 to -0.70) Moderate positive (negative) correlation 0.30 to 0.50 (-0.30 to -0.50) Weak positive (negative) correlation 0.00 to 0.30 (0.00 to -0.30) Very weak positive (negative) correlation Note: the cor function can also be used to compute the correlation coefficient. For more details type ?cor into the R console. The next step in our exploratory data analysis is to visualise the data using appropriate plotting techniques. Here, a scatterplot is appropriate since both score and bty_avg are numerical variables: ggplot(evals.scores, aes(x = bty_avg, y = score)) + geom_point() Figure 2: Relationship between teaching and beauty scores. Task: Update the above code to give the plot more appropriate axes labels and a title similar to the scatterplot above. Hint You need to include the labs layer and provide it with the x, y and title arguments. Note: the outcome variable should always be plotted on the y-axis. Solution ggplot(evals.scores, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship of teaching and beauty scores&quot;) What can we observe from the scatterplot? Well, here it can be hard to see the weakly positive linear relationship suggested by the correlation coefficient (0.187), which is why our correlation coefficient is considered very weak in the verbal interpretation. Additionally, as our numerical variables are averages of integers (or whole numbers), a lot of the values will be plotted on top of one another. Remember, from Week 1, that this is referred to as over-plotting, and can be alleviated by slightly nudging (jittering) the points in a random direction. For example, let's look at the three points in the top-right of the scatterplot that have a beauty score slightly less than 8. Are there really only three values plotted there, or are there more that we cannot see due to over-plotting? Let's find out by adding some jitter to the plot: Figure 3: Comparing regular and jittered scatterplots. From the jittered scatterplot we can see that: There are actually more than just three points plotted in the top-right; and There are more instructors with a beauty score between 3 and 4.5 than originally appears due to over-plotting. Note: jittering does not actually change the values within a data set, it is merely a tool for visualisation purposes. Hence, we shall continue on with plotting the original data. Formal analysis After completing an exploratory data analysis the next step is to perform a formal analysis on the data. This involves constructing an appropriate statistical model from the information gathered during the exploratory data analysis step. Here, we shall be fitting a simple linear regression model to the data on teaching and beauty scores, where our objective is to acquire the best fitting regression line. This is done by finding estimates of the intercept (\\(\\alpha\\)) and slope (\\(\\beta\\)) which give us the best-fitting line to the data. This can be done in R using the lm function to fit a linear model to the data as follows: model &lt;- lm(score ~ bty_avg, data = evals.scores) model Call: lm(formula = score ~ bty_avg, data = evals.scores) Coefficients: (Intercept) bty_avg 3.88034 0.06664 This tells us that our best-fitting line to the data is: \\[\\widehat{\\mbox{score}} = \\widehat{\\alpha} + \\widehat{\\beta} x_i = 3.88 + 0.067 \\cdot \\mbox{bty_avg}\\] where \\(\\widehat{\\alpha} = 3.880338\\) is the intercept coefficient and means that, for any instructor with a bty_avg = 0, their average teaching score would be 3.8803. Note that bty_avg = 0 is not actually possible as bty_avg is an average of beauty scores ranging between 1 and 10. \\(\\widehat{\\beta} = 0.066637\\) is the slope coefficient associated with the exploratory variable bty_avg, and summarises the relationship between score and bty_avg. That is, as bty_avg increases, so does score, such that For every 1 unit increase in bty_avg, there is an associated increase of, on average, 0.067 units of score. Finally, we can superimpose our best-fitting line onto our scatterplot to see how it fits through the points using the geom_smooth function as follows: ggplot(evals.scores, aes(x = bty_avg, y = score)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Teaching Score&quot;, title = &quot;Relationship of teaching and beauty scores&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Figure 4: Relationship between teaching and beauty scores with regression line superimposed. Now that we have fitted our simple linear regression model to the data, how do we use it to obtain information on individual data points? This can be done by looking at the fitted values. For example, let's say we are interested in looking at the 21st instructor who has the following teaching and beauty scores: score bty_avg 4.9 7.33 What would the score be on our best-fitting line for this instructor with a bty_avg of 7.33? We simply plug the instructor's bty_avg into our regression model: \\[\\widehat{\\mbox{score}} = 3.88034 + 0.06664 \\cdot \\mbox{bty_avg} = 3.88034 + 0.06664 \\cdot 7.33 = 4.369,\\] The regression model gives our instructor a score of 4.369. However, we know the score of the instructor is 4.9 meaning that our model was out by 0.531. This is known as the residual (\\(\\epsilon\\)) and can be thought of as the error or lack of fit of the regression line. In this case, the residual is given by: \\[ \\widehat{\\epsilon} = y - \\widehat{y} = 4.9 - 4.369 = 0.531.\\] This is essentially the distance between the fitted regression line and the observed (true) value. This can be seen on the following scatterplot: `geom_smooth()` using formula &#39;y ~ x&#39; Figure 5: Example of observed value, fitted value, and residual. where the red circle is the observed (true) score (\\(y=4.9\\)) of the instructor; the red square is the fitted value (\\(\\widehat{y} = 4.369\\)) from the regression line; and the blue arrow is the distance between the observed and fitted values, that is, the residual. Task: Obtain the observed score and bty_avg for the 27th instructor. Hint Pick out the 27th row of the evals.scores data set. Solution evals.scores[27, ] # A tibble: 1 x 2 score bty_avg &lt;dbl&gt; &lt;dbl&gt; 1 4.5 5.5 Using the regression model, find the fitted value of score for the 13th instructor? 4.6 4.1 3.2 3.9 Using the regression model, find the value of the residual error for the 56th instructor? 0.41 0.35 0.28 0.31 To obtain the fitted values and residuals for all instructors within the data set we can use the get_regression_points function: regression.points &lt;- get_regression_points(model) regression.points # A tibble: 463 x 5 ID score bty_avg score_hat residual &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 4.7 5 4.21 0.486 2 2 4.1 5 4.21 -0.114 3 3 3.9 5 4.21 -0.314 4 4 4.8 5 4.21 0.586 5 5 4.6 3 4.08 0.52 6 6 4.3 3 4.08 0.22 7 7 2.8 3 4.08 -1.28 8 8 4.1 3.33 4.10 -0.002 9 9 3.4 3.33 4.10 -0.702 10 10 4.5 3.17 4.09 0.409 # ... with 453 more rows # i Use `print(n = ...)` to see more rows The table provides us with information on the: score: the observed value of the outcome variable \\(y\\); bty_avg: the values of the explanatory variable \\(x\\); score_hat: the values of the fitted values \\(\\widehat{y}\\); and residual: the residuals \\(y - \\widehat{y}\\). Using the table above, find the fitted value of score for the 72nd instructor? 4.3 3.7 3.9 4.2 Using the table above, find the value of the residual error for the 44th instructor? 0.66 0.61 0.55 0.59 Assessing model fit When we fit a simple linear regression model there are five main assumptions that we need to hold true in order for the model to be an appropriate fit to the data. These assumptions are: The deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero. The scale of the variability of the residuals is constant at all values of the explanatory variables. The residuals are normally distributed. The residuals are independent. The values of the explanatory variables are recorded without error. One way we can check our first assumption is to plot the residuals (residuals) against the explanatory variable (bty_avg). From this we should be able to check that the explanatory variable has a linear relationship with the outcome variable (score). We can plot the residuals against our explanatory variable using: ggplot(regression.points, aes(x = bty_avg, y = residual)) + geom_point() + labs(x = &quot;Beauty Score&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) Figure 6: Residuals against beauty score. Ideally, for the first assumption to hold we should observe the following: There should be no systematic pattern, i.e. the residuals should appear randomly scattered. The residuals should have mean zero. That is, they should be evenly scattered above and below the zero line. This is because the regression model will overestimate some of the fitted values, but it will also underestimate some, and hence, on average, they should even out to have mean zero. Does the first assumption appear to hold from the scatterplot of the residuals against the explanatory variable? Yes No We can examine our first two assumptions by also plotting the residuals against the fitted values as follows: ggplot(regression.points, aes(x = score_hat, y = residual)) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) Figure 7: Residuals against fitted values. From the plot of the residuals against the fitted values we want to examine whether: The residuals have mean zero. If the residuals have constant variance across all levels of the fitted values. That is, the range (or spread) of the residuals should be similar across all levels of the fitted values and display no obvious changes in variability. Do assumptions 1. and 2. appear to hold from the plot of the residuals against the fitted values? Yes No Task: Examine the following residual plots before answering the questions below. Figure 8: Examples of different residual patterns. Do the assumptions hold for Example 1? Yes No Do the assumptions hold for Example 2? Yes No To assess our third assumption that the residuals are normally distributed we can simply plot a histogram of the residuals: ggplot(regression.points, aes(x = residual)) + geom_histogram(binwidth = 0.25, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) Figure 9: Histogram of residuals. Ideally, for the assumption of normally distributed residuals, the histogram should be bell-shaped and centred at zero, i.e. the residuals have mean zero. However, in practice this will almost never be the case, and as such, like the plots of the residuals, there is some subjectivity in whether you believe the assumptions hold. For instance, here we can see that the histogram is slightly skewed to the left in that the distribution has a longer tail to the left. However, in my opinion, this is not of much concern as the histogram appears to be relatively symmetrical and bell-shaped, and as such the assumption of normally distributed random errors appears valid in this case. Finally, assumptions 4. and 5. are often justified on the basis of the experimental context and are not formally examined. "],["simple-linear-regression-with-one-categorical-explanatory-variable.html", "Simple linear regression with one categorical explanatory variable Exploratory data analysis Formal analysis Assessing model fit", " Simple linear regression with one categorical explanatory variable Here, we will fit a simple linear regression model were the explanatory variable is categorical. A categorical variable is a variable of a fixed number of possible values, assigning units to particular groups (or categories) based on qualitative properties. We shall examine the gapminder data set from the gapminder library. This is a data set on life expectancy across various countries around the world. We will explore life expectancy and its potential differences: Between continents: Does life expectancy vary, on average, between the five continents of the world?; and Within continents: Does life expectancy vary, on average, within the five continents of the world? Thus, we will be looking at: life expectancy as our numerical outcome variable \\(y\\); and the continent a country is within as our categorical variable \\(x\\). Exploratory data analysis Let's examine a subset of the gapminder data set relating to the year 2007. That is, we use the filter function to choose only the observations pertaining to 2007, and then select the variables we are interested in: gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% select(country, continent, lifeExp) The new data set can be examined using either the View or glimpse functions, i.e. glimpse(gapminder2007) Rows: 142 Columns: 3 $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, ~ $ continent &lt;fct&gt; Asia, Europe, Africa, Africa, Americas, Oceania, Europe, Asi~ $ lifeExp &lt;dbl&gt; 43.828, 76.423, 72.301, 42.731, 75.320, 81.235, 79.829, 75.6~ Here, we can see that both country and continent are factors (fct), which is how R stores categorical variables. Similarly to our previous exploratory data analysis, we can obtain summary statistics using the skim function. First, let's take a look at the life expectancy (lifeExp) and continent variables: gapminder2007 %&gt;% select(continent, lifeExp) %&gt;% skim() Table 2: Data summary Name Piped data Number of rows 142 Number of columns 2 _______________________ Column type frequency: factor 1 numeric 1 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts continent 0 1 FALSE 5 Afr: 52, Asi: 33, Eur: 30, Ame: 25 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist lifeExp 0 1 67.01 12.07 39.61 57.16 71.94 76.41 82.6  The summary output for the numerical outcome variable lifeExp is the same as we have seen previously. However, for the categorical variable continent we obtain: n_unique: the number of levels (or categories) of the variable, i.e. the number of continents. top_counts: the top counts from the top categories. ordered: whether the variable is ordinal or not. That is, whether or not the ordering of the categories matter. From the gapminder data set, how many countries comprise Asia? 52 33 25 30 Which continent has the third lowest number of countries in the gapminder data set? Africa Americas Oceania Europe Asia Is the continent variable ordinal? Yes No We can summarise any differences in life expectancy by continent by taking a look at the median and mean life expectancies of each continent using the group_by and summarize functions as follows: lifeExp.continent &lt;- gapminder2007 %&gt;% group_by(continent) %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) # A tibble: 5 x 3 continent median mean &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Africa 52.9 54.8 2 Americas 72.9 73.6 3 Asia 72.4 70.7 4 Europe 78.6 77.6 5 Oceania 80.7 80.7 Which continent has the lowest life expectancy? Asia Americas Oceania Africa Europe Task: Obtain the worldwide median and mean life expectancies from the gapminder2007 data set. Hint Use the median and mean functions within the summarize function. Solution gapminder2007 %&gt;% summarize(median = median(lifeExp), mean = mean(lifeExp)) # A tibble: 1 x 2 median mean &lt;dbl&gt; &lt;dbl&gt; 1 71.9 67.0 Is the average life expectancy of Europe higher or lower than the worldwide average? Lower Higher Boxplots are often used when examining the distribution of a numerical outcome variable across different levels of a categorical variable: ggplot(gapminder2007, aes(x = continent, y = lifeExp)) + geom_boxplot() + labs(x = &quot;Continent&quot;, y = &quot;Life expectancy (years)&quot;, title = &quot;Life expectancy by continent&quot;) Figure 10: Life expectancy by continent in 2007. Here, we can see that the middle 50% of the life expectancy distribution of Africa is much smaller than, and does not overlap with, the middle 50% of the remaining four continents, while the country with the highest life expectancy in Africa is less than all countries in Oceania. Speaking of Oceania, there is almost no variability (or spread) in life expectancy in this continent, however that may well be because it consists of only two countries (Australia and New Zealand). There is more variability in life expectancy in the continents of Africa and Asia. What country in Asia has a much lower life expectancy than the rest of the continent? Iran Bahrain Cambodia China Afghanistan Other Hint Start by subsetting the gapminder2007 data set for only the continent of Asia using the filter function. The group_by and summarize functions can then be used to obtain a table of the median and mean life expectancies of all countries in Asia. Formal analysis When examining the relationship between a numerical outcome variable \\(y\\) and a categorical explanatory variable \\(x\\), we are not just looking to find the best-fitting line to the data as before, but are examining relative differences to a baseline category. For example, the table below displays the mean life expectancy of each continent, as well as the differences between the means of each continent and Africa. Now, in comparison with Africa we can see that the mean life expectancy of the other continents is around 18-26 years greater than that of Africa. continent mean mean vs Africa Africa 54.81 0.00 Americas 73.61 18.80 Asia 70.73 15.92 Europe 77.65 22.84 Oceania 80.72 25.91 Now let us fit our regression model to the data, where lifeExp is our outcome variable \\(y\\) and continent is our categorical explanatory variable \\(x\\): lifeExp.model &lt;- lm(lifeExp ~ continent, data = gapminder2007) # A tibble: 5 x 7 term estimate std_error statistic p_value lower_ci upper_ci &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 intercept 54.8 1.02 53.4 0 52.8 56.8 2 continent: Americas 18.8 1.8 10.4 0 15.2 22.4 3 continent: Asia 15.9 1.65 9.68 0 12.7 19.2 4 continent: Europe 22.8 1.70 13.5 0 19.5 26.2 5 continent: Oceania 25.9 5.33 4.86 0 15.4 36.4 Note: For now we will ignore the last five columns of the regression table and only focus on the estimate column. We obtain five estimates: the intercept term and four others relating to the continents (continentAmericas, continentAsia, continentEurope and continentOceania), such that our regression equation is given as: \\[\\widehat{\\mbox{life exp}} = \\widehat{\\alpha} + \\widehat{\\beta}_{\\mbox{Amer}} \\cdot \\mathbb{I}_{\\mbox{Amer}}(x) + \\widehat{\\beta}_{\\mbox{Asia}} \\cdot \\mathbb{I}_{\\mbox{Asia}}(x) + \\widehat{\\beta}_{\\mbox{Euro}} \\cdot \\mathbb{I}_{\\mbox{Euro}}(x) + \\widehat{\\beta}_{\\mbox{Ocean}} \\cdot \\mathbb{I}_{\\mbox{Ocean}}(x),\\] where the intercept \\(\\widehat{\\alpha}\\) is the mean life expectancy for our baseline category Africa; \\(\\widehat{\\beta}_{\\mbox{continent}}\\) is the difference in the mean life expectancy of a given continent relative to the baseline category Africa; and \\(\\mathbb{I}_{\\mbox{continent}}(x)\\) is an indicator function such that \\[\\mathbb{I}_{\\mbox{continent}}(x)=\\left\\{ \\begin{array}{ll} 1 ~~~ \\mbox{if country} ~ x ~ \\mbox{is in the continent},\\\\ 0 ~~~ \\mbox{Otherwise}.\\\\ \\end{array} \\right.\\] Essentially, the estimates for each continent are known as offsets relative to the baseline category (Africa in this case). For example, the mean life expectancy for Africa is simply equal to the intercept term \\(\\widehat{\\alpha} = 54.8\\). However, the mean life expectancy for Asia is \\(\\widehat{\\alpha} + \\widehat{\\beta}_{\\mbox{Asia}} \\cdot \\mathbb{I}_{\\mbox{Asia}}(x) = 54.8 + 15.9 \\cdot 1 = 70.7\\). From the regression model, what is the mean life expectancy of the Americas? 80.7 77.6 73.6 Assessing model fit What do the fitted values \\(\\widehat{y}\\) and the residuals \\(y - \\widehat{y}\\) correspond to when we are dealing with a categorical explanatory variable? Let's explore the gapminder2007 data set in order to understand how they work. # A tibble: 142 x 3 country continent lifeExp &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; 1 Afghanistan Asia 43.8 2 Albania Europe 76.4 3 Algeria Africa 72.3 4 Angola Africa 42.7 5 Argentina Americas 75.3 6 Australia Oceania 81.2 7 Austria Europe 79.8 8 Bahrain Asia 75.6 9 Bangladesh Asia 64.1 10 Belgium Europe 79.4 # ... with 132 more rows # i Use `print(n = ...)` to see more rows Here, we see the life expectancy of each country and the continent they are from. For example, let's remember the life expectancies of Afghanistan (43.8) and Bahrain (75.6). Now, we can obtain the fitted values and residuals in the same way we did previously: regression_points &lt;- get_regression_points(lifeExp.model) # A tibble: 142 x 5 ID lifeExp continent lifeExp_hat residual &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 43.8 Asia 70.7 -26.9 2 2 76.4 Europe 77.6 -1.23 3 3 72.3 Africa 54.8 17.5 4 4 42.7 Africa 54.8 -12.1 5 5 75.3 Americas 73.6 1.71 6 6 81.2 Oceania 80.7 0.516 7 7 79.8 Europe 77.6 2.18 8 8 75.6 Asia 70.7 4.91 9 9 64.1 Asia 70.7 -6.67 10 10 79.4 Europe 77.6 1.79 # ... with 132 more rows # i Use `print(n = ...)` to see more rows The first row of the regression table corresponds to the observed life expectancy (lifeExp), fitted value (lifeExp_hat) and the residual error (residual) for Afghanistan. Here, we see that the fitted value (lifeExp_hat = 70.7) is much greater than the life expectancy of Afghanistan (lifeExp = 43.8) with a residual = -26.9. Now, for Bahrain (ID = 8) we also have the same fitted value (lifeExp_hat = 70.7). This is because the fitted values for each country correspond to the mean life expectancy for that continent. Hence, all countries in Africa have the fitted valuelifeExp_hat = 54.8, while all countries in Europe have the fitted value lifeExp_hat = 77.6. The residual error in this case is then how much a country deviates from the mean life expectancy of its respective continent. Which country has the largest positive residual error? Reunion Belgium Argentina Other Saudi Arabia Hint This can be found by looking through the regression table above. You could also try using the which.max function on the residual column. For assessing the assumptions surrounding the residuals for a categorical explanatory variable, we can plot the residuals for each continent: ggplot(regression_points, aes(x = continent, y = residual)) + geom_jitter(width = 0.1) + labs(x = &quot;Continent&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;) Figure 11: Residuals over continent. Note: we have jittered the points for each continent in order to see the residuals for each country more clearly. Here, we see that there is an even spread of the residuals above and below the zero line for each continent, and hence our assumption that the residuals have mean zero appears valid. There is an outlier observed for Asia with a large negative residual (relating to Afghanistan). To check that the residual errors are normally distributed, we plot a histogram of them: ggplot(regression_points, aes(x = residual)) + geom_histogram(binwidth = 5, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) Figure 12: Histogram of residuals. Are the residuals normally distributed with mean zero? Yes No "],["further-tasks-simple-linear-regression.html", "Further Tasks: Simple Linear Regression", " Further Tasks: Simple Linear Regression How do we commonly represent an outcome variable? \\(x\\) \\(y\\) What is another name for the outcome varable? Independent Predictor Response Covariate For the linear regression model \\(y_i = \\alpha + \\beta x_i + \\epsilon_i, ~ \\epsilon_i \\sim N(0, \\sigma^2)\\), what term refers to the slope of the regression line? \\(\\alpha\\) \\(y\\) \\(\\epsilon\\) \\(\\beta\\) What would be the verbal interpretation of a correlation coefficient of 0.73? Moderate negative correlation Very strong positive correlation Strong negative correlation Strong positive correlation Weak positive correlation What does \\(y - \\widehat{y}\\) refer to? Observed value Residual error Fitted value A factor in R is what type of variable? Categorical Numerical Task Examine the relationship between teaching score and age in the evals data set. What is the value of the correlation coefficient? How would you interpret this verbally? Finally, produce a scatterplot of teaching score and age. Hint Start by looking at a subset of the evals data set referring to score and age. You can then use the get_correlation function to obtain the correlation coefficient. The scatterplot can be produced using ggplot and geom_point. Note, remember to give your scatterplot appropriate labels. Task Perform a formal analysis of the relationship between teaching score and age by fitting a simple linear regression model. Superimpose your best-fitting line onto your scatterplot from Task 2. Hint Use the lm function to fit the regression model. Check the geom_smooth function to add the best-fitting line to your scatterplot. What is the fitted value of the 22nd instructor? 4.3 4.1 4.6 5.3 What is the residual error for the 34th instructor? 0.1 -0.15 4.2 -0.07 4.27 Task Assess the model assumptions from Task 3 by plotting the residuals against the explanatory variable and fitted values, respectively. Also, plot a histogram of the residuals to assess whether they are normally distributed. Hint You will want to store the fitted values and residuals from your model using the get_regression_points function. The plots examining the assumptions can then be plotted using ggplot, geom_point and geom_histogram. Do the model assumptions hold? Yes No Task If we were interested in exploring the relationship between teaching score and age for male and female professors separately we could produce a scatterplot of teaching score and age with different symbols (and an appropriate legend) for males and females. Create this plot and superimpose on it the linear regression models fitted to the male and female data separately. Note you do not need to examine the models formally, we'll look at that in a future lab. Hint The aes function in ggplot2 can take arguments such asshape=..., color=... and linetype=... to define the appearance of plotted objects. Task Perform the same analysis we did on life expectancy from the gapminder data set in 2007. However, subset the data for the year 1997. Are there any differences in the results across this 10 year period? Hint Start by subsetting the gapminder data set for the year 1997 using the filter function. "],["multiple-linear-regression.html", "Multiple Linear Regression", " Multiple Linear Regression We have already introduced regression modelling where we modeled the relationship between an outcome variable \\(y\\) and a single explanatory variable \\(x\\). We only included one explanatory variable \\(x\\), which was either a continuous or a categorical variable. Now, we shall examine fitting regression models with more than one explanatory variable. This is known as multiple regression. When fitting regression models with multiple explanatory variables, the interpretation of an explanatory variable is made in association with the other variables. For example, if we wanted to model income then we may consider an individual's level of education, and perhaps the wealth of their parents. Then, when interpreting the effect an individuals level of education has on their income, we would also be considering the effect of the wealth of their parents simultaneously, as these two variables are likely to be related. Create a .Rmd file to load the following packages into R: library(ggplot2) library(dplyr) library(moderndive) library(ISLR) library(skimr) library(plotly) library(tidyr) library(jtools) Additional information and examples can be found in Chapter 6 of Statistical Inference via Data Science. "],["regression-modelling-with-two-continuous-explanatory-variables.html", "Regression modelling with two continuous explanatory variables Exploratory data analysis Formal analysis Assessing model fit", " Regression modelling with two continuous explanatory variables Let's start by fitting a regression model with two continuous explanatory variables. We shall examine a data set within the ISLR package, which is an accompanying R package related to the textbook An Introduction to Statistical Learning with Applications in R. We will take a look at the Credit data set, which consists of predictions made on the credit card balance of 400 individuals, where the predictions are based on information relating to income, credit limit and the level of education of an individual. Note: This is a simulated data set and is not based on credit card balances of actual individuals. The regression model we will be considering contains the following variables: the continuous outcome variable \\(y\\), the credit card balance of an individual; and two explanatory variables \\(x_1\\) and \\(x_2\\), which are an individual's credit limit and income (both in thousands of dollars), respectively. Exploratory data analysis Task: Start by subsetting the Credit data set so that we only have the variables we are interested in, that is, Balance, Limit and Income. Note, it is best to give your new data set a different name than Credit as to not overwrite the original Credit data set. Solution Cred &lt;- Credit %&gt;% select(Balance, Limit, Income) Your new data set should look like the one below. Balance Limit Income 1 333 3606 14.891 2 903 6645 106.025 3 580 7075 104.593 4 964 9504 148.924 5 331 4897 55.882 6 1151 8047 80.180 We can also use the glimpse function to take a look at our new data set (named Cred in this case): glimpse(Cred) Rows: 400 Columns: 3 $ Balance &lt;int&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0, 2~ $ Limit &lt;int&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, 81~ $ Income &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 71.~ Note: the View function can also be used within RStudio to examine a spreadsheet of the data. Now, let's take a look at summary statistics relating to our data set using the skim function: Cred %&gt;% skim() Table 3: Data summary Name Piped data Number of rows 400 Number of columns 3 _______________________ Column type frequency: numeric 3 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Balance 0 1 520.02 459.76 0.00 68.75 459.50 863.00 1999.00  Limit 0 1 4735.60 2308.20 855.00 3088.00 4622.50 5872.75 13913.00  Income 0 1 45.22 35.24 10.35 21.01 33.12 57.47 186.63  What is the mean credit limit? $2308.20 $4735.60 $4622.50 $5872.75 What is the median credit balance? $863.00 $459.76 $520.01 $459.50 What percentage of credit card holders have an income greater than $57,470? 25% 50% 75% 100% Now that we are looking at the relationship between an outcome variable and multiple explanatory variables, we need to examine the correlation between each of them. We can examine the correlation between Balance, Limit and Income by creating a table of correlations as follows: Cred %&gt;% cor() Balance Limit Income Balance 1.0000000 0.8616973 0.4636565 Limit 0.8616973 1.0000000 0.7920883 Income 0.4636565 0.7920883 1.0000000 What is the correlation coefficient for the linear relationship between Balance and Limit? 0.464 0.862 0.792 1.000 What would be the verbal interpretation of the correlation coefficient for the linear relationship between Balance and Income? Weakly positive Moderately positive Weakly negative Strongly positive Question: Why are the diagonal components of our correlation table all equal to 1? From our correlation table we can see that the correlation between our two explanatory variables is 0.792, which is a strong positive linear relationship. Hence, we say there is a high degree of collinearity between our explanatory variables. Collinearity (or multicollinearity) occurs when an explanatory variable within a multiple regression model can be linearly predicted from the other explanatory variables with a high level of accuracy. For example, in this case, since Limit and Income are highly correlated, we could take a good guess as to an individual's Income based on their Limit. That is, having one or more highly correlated explanatory variables within a multiple regression model essentially provides us with redundant information. Normally, we would remove one of the highly correlated explanatory variables, however, for the purpose of this example we shall ignore the potential issue of collinearity and carry on. You may want to use the pairs function or the ggpairs function from the GGally package to look at potential relationships between all of the variables within a data set. Let's now produce scatterplots of the relationship between the outcome variable and the explanatory variables. First, we shall look at the scatterplot of Balance against Limit: ggplot(Cred, aes(x = Limit, y = Balance)) + geom_point() + labs(x = &quot;Credit limit (in $)&quot;, y = &quot;Credit card balance (in $)&quot;, title = &quot;Relationship between balance and credit limit&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Figure 13: Relationship between balance and credit limit. What is the relationship between balance and credit limit? Negative Positive None Now, let's look at a scatterplot of Balance and Income: ggplot(Cred, aes(x = Income, y = Balance)) + geom_point() + labs(x = &quot;Income (in $1000)&quot;, y = &quot;Credit card balance (in $)&quot;, title = &quot;Relationship between balance and income&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Figure 14: Relationship between balance and income. What is the relationship between balance and income? Negative Positive None The two scatterplots above focus on the relationship between the outcome variable Balance and each of the explanatory variables independently. In order to get an idea of the relationship between all three variables we can use the plot_ly function within the plotly library to plot a 3-dimensional scatterplot as follows (you may need to hover over/click on the area below for the plot to appear): plot_ly(Cred, x = ~Income, y = ~Limit, z = ~Balance, type = &quot;scatter3d&quot;, mode = &quot;markers&quot;) Figure 15: 3D scatterplot of balance, credit limit, and income. Click on and drag Figure 3 to change the viewing perspective. Is it reasonable to think about the points being randomly scattered around a two dimensional plane? Before, when we fitted a regression model with one continuous explanatory variable, we were looking at the best-fitting line. However, now that we have more than one explanatory variable, we are looking at the best-fitting plane, which is a 3-dimensional generalisation of the best-fitting line. Formal analysis The multiple regression model we will be fitting to the credit balance data is given as: \\[y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i, ~~~~ \\epsilon \\sim N(0, \\sigma^2),\\] where \\(y_i\\) is the credit balance of the \\(i^{th}\\) individual; \\(\\alpha\\) is the intercept and positions the best-fitting plane in 3D space; \\(\\beta_1\\) is the coefficient for the first explanatory variable \\(x_1\\); \\(\\beta_2\\) is the coefficient for the second explanatory variable \\(x_2\\); and \\(\\epsilon_i\\) is the \\(i^{th}\\) random error component. Similarly to simple linear regression, we use the lm function to fit the regression model and the get_regression_table function to view our parameter estimates: Balance.model &lt;- lm(Balance ~ Limit + Income, data = Cred) get_regression_table(Balance.model) # A tibble: 3 x 7 term estimate std_error statistic p_value lower_ci upper_ci &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 intercept -385. 19.5 -19.8 0 -423. -347. 2 Limit 0.264 0.006 45.0 0 0.253 0.276 3 Income -7.66 0.385 -19.9 0 -8.42 -6.91 Notes To include multiple explanatory variables within a regression model we simply use the + sign, that is Balance ~ Limit + Income. An alternative to get_regression_table is the summ function in the jtools package which allows a lot more control over what is included in the summary table. Here is the default output... summ(Balance.model) Observations 400 Dependent variable Balance Type OLS linear regression F(2,397) 1341.54 R² 0.87 Adj. R² 0.87 Est. S.E. t val. p (Intercept) -385.18 19.46 -19.79 0.00 Limit 0.26 0.01 44.95 0.00 Income -7.66 0.39 -19.90 0.00 Standard errors: OLS How do we interpret our model estimates defining the regression plane? They can be interpreted as follows: The intercept represents the credit card balance (Balance) of an individual who has $0 for both credit limit (Limit) and income (Income). However, this interpretation, though technically correct, is nonsensical in this context as there are no credit cards with $0 credit limit and no people with an income of $0 in the data set. In contexts where the intercept term does not have a meaningful interpretation, we think of it as a value that positions the fitted model with no intuitive meaning. The coefficient for credit limit (Limit) tells us that, taking all other variables in the model into account and holding them constant, there is an associated increase, on average, in credit card balance of $0.26 for every $1 increase in credit limit. Similarly, the coefficient for income (Income) tells us that, taking all other variables in the model into account and holding them constant, there is an associated decrease, on average, in credit card balance of $7.66 for every $1 increase in income. What do you notice that is strange about our coefficient estimates given our exploratory data analysis? Well, from our scatterplots of credit card balance against both credit limit and income, we saw that there appeared to be a positive linear relationship. Why do we then get a negative coefficient for income (-7.66)? This is due to a phenomenon known as Simpson's Paradox. This occurs when there are trends within different categories (or groups) of data, but that these trends disappear when the categories are grouped as a whole. For more details see Section 7.3.2 of An Introduction to Statistical and Data Sciences in R. Assessing model fit Now we need to assess our model assumptions. As a reminder, our model assumptions are: The deterministic part of the model captures all the non-random structure in the data, i.e. the residuals have mean zero. The scale of the variability of the residuals is constant at all values of the explanatory variables. The residuals are normally distributed. The residuals are independent. The values of the explanatory variables are recorded without error. First, we need to obtain the fitted values and residuals from our regression model: regression.points &lt;- get_regression_points(Balance.model) # A tibble: 400 x 6 ID Balance Limit Income Balance_hat residual &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 333 3606 14.9 454. -121. 2 2 903 6645 106. 559. 344. 3 3 580 7075 105. 683. -103. 4 4 964 9504 149. 986. -21.7 5 5 331 4897 55.9 481. -150. 6 6 1151 8047 80.2 1127. 23.6 7 7 203 3388 21.0 349. -146. 8 8 872 7114 71.4 948. -76.0 9 9 279 3300 15.1 371. -92.2 10 10 1350 6819 71.1 873. 477. # ... with 390 more rows # i Use `print(n = ...)` to see more rows Recall that get_regression_points provides us with values of the: outcome variable \\(y\\) (Balance); explanatory variables \\(x_1\\) (Limit) and \\(x_2\\) (Income); fitted values \\(\\widehat{y}\\); and the residual error (\\(y - \\widehat{y}\\)). We can assess our first two model assumptions by producing scatterplots of our residuals against each of our explanatory variables. First, let's begin with the scatterplot of the residuals against credit limit: ggplot(regression.points, aes(x = Limit, y = residual)) + geom_point() + labs(x = &quot;Credit limit (in $)&quot;, y = &quot;Residual&quot;, title = &quot;Residuals vs credit limit&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) Figure 16: Residuals vs credit limit. Do the first two model assumptions appear to hold from the scatterplot of the residuals against credit limit? Yes No Now, let's plot a scatterplot of the residuals against income: ggplot(regression.points, aes(x = Income, y = residual)) + geom_point() + labs(x = &quot;Income (in $1000)&quot;, y = &quot;Residual&quot;, title = &quot;Residuals vs income&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) Figure 17: Residuals vs income. Do the first two model assumptions appear to hold from the scatterplot of the residuals against income? Yes No Finally, we can check if the residuals are normally distributed by producing a histogram: ggplot(regression.points, aes(x = residual)) + geom_histogram(color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 18: Figure 6: Histogram of the residuals. Do the residuals appear to be normally distributed and centred at zero? Yes No "],["regression-modelling-with-one-continuous-and-one-categorical-explanatory-variable.html", "Regression modelling with one continuous and one categorical explanatory variable Exploratory data analysis Multiple regression: parallel slopes model Multiple regression: interaction model Assessing model fit", " Regression modelling with one continuous and one categorical explanatory variable Let's revisit the instructor evaluation data set evals we used for simple linear regression. You were tasked with examining the relationship between teaching score (score) and age (age). Now, let's also introduce the additional (binary) categorical explanatory variable gender (gender). That is, we will be examining: the teaching score (score) as our outcome variable \\(y\\); age (age) as our continuous explanatory variable \\(x_1\\); and gender (gender) as our categorical explanatory variable \\(x_2\\). Exploratory data analysis Task: Start by subsetting the evals data set so that we only have the variables we are interested in, that is, score, age and gender. Note, it is best to give your new data set a different name than evals as to not overwrite the original evals data set. Solution eval.score &lt;- evals %&gt;% select(score, age, gender) Your new data set should look like the one below. # A tibble: 6 x 3 score age gender &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; 1 4.7 36 female 2 4.1 36 female 3 3.9 36 female 4 4.8 36 female 5 4.6 59 male 6 4.3 59 male Note: You can also view your data set using the glimpse function, or by opening a spreadsheet view in RStudio using the View function. We can use the skim function to obtain some summary statistics from our data: eval.score %&gt;% skim() Table 4: Data summary Name Piped data Number of rows 463 Number of columns 3 _______________________ Column type frequency: factor 1 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts gender 0 1 FALSE 2 mal: 268, fem: 195 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.3 3.8 4.3 4.6 5  age 0 1 48.37 9.80 29.0 42.0 48.0 57.0 73  How many males are in the data set? 195 463 268 What is the median age in the data set? 57.00 48.37 48.00 What is the maximum teaching score of the bottom 25% of professors? 2.3 4.3 3.8 Now, let's compute the correlation coefficient between our outcome variable score and our continuous explanatory variable age: eval.score %&gt;% get_correlation(formula = score ~ age) # A tibble: 1 x 1 cor &lt;dbl&gt; 1 -0.107 Note: The correlation coefficient only exists between continuous variables, which is why we do not include our categorical variable gender. What is the verbal interpretation of the correlation coefficient? Moderate positive Very weak negative Weak negative Very weak positive We can now visualise our data by producing a scatterplot, where seeing as we have the categorical variable gender, we shall plot the points using different colours for each gender: ggplot(eval.score, aes(x = age, y = score, color = gender)) + geom_jitter() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Figure 19: Instructor evaluation scores by age and gender. The points have been jittered. Note: The above code has jittered the points, however, this is not necessary and geom_point would suffice. To plot separate points by gender we simply add the color argument to the aes function and pass to it gender. From the scatterplot we can see that: There are very few women over the age of 60 in our data set. From the plotted regression lines we can see that the lines have different slopes for men and women. That is, the associated effect of increasing age appears to be more severe for women than it does for men, i.e. the teaching score of women drops faster with age. Multiple regression: parallel slopes model Here, we shall begin by fitting what is referred to as a parallel regression lines model. This model implies that the slope of relationship between teaching score (score) and age (age) is the same for both males and females, with only the intercept of the regression lines changing. Hence, our parallel regression lines model is given as: \\[\\begin{align} y_{i} &amp;= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i \\nonumber \\\\ &amp;= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age}_i + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(i) + \\epsilon_i, \\nonumber \\end{align}\\] where \\(\\alpha\\) is the intercept of the regression line for females; \\(\\beta_{\\mbox{age}}\\) is the slope of the regression line for both males and females; \\(\\mbox{age}_i\\) is the age of the \\(i\\)th observation \\(\\beta_{\\mbox{male}}\\) is the additional term added to \\(\\alpha\\) to get the intercept of the regression line for males; and \\(\\mathbb{I}_{\\mbox{male}}(i)\\) is an indicator function such that \\[\\mathbb{I}_{\\mbox{male}}(i)=\\left\\{ \\begin{array}{ll} 1 ~~~ \\mbox{if the} ~ i \\mbox{th observation is male},\\\\ 0 ~~~ \\mbox{Otherwise}.\\\\ \\end{array} \\right.\\] We can fit the parallel regression lines model as follows: par.model &lt;- lm(score ~ age + gender, data = eval.score) get_regression_table(par.model) # A tibble: 3 x 7 term estimate std_error statistic p_value lower_ci upper_ci &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 intercept 4.48 0.125 35.8 0 4.24 4.73 2 age -0.009 0.003 -3.28 0.001 -0.014 -0.003 3 gender: male 0.191 0.052 3.63 0 0.087 0.294 Hence, the regression line for females is given by: \\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age},\\] while the regression line for males is given by: \\[\\widehat{\\mbox{score}} = 4.48 - 0.009 \\cdot \\mbox{age} + 0.191 = 4.671 - 0.009 \\cdot \\mbox{age}.\\] From the parallel regression lines model, what would be the teaching score of a female instructor aged 37? 4.338 4.246 4.147 4.451 From the parallel regression lines model, what would be the teaching score of a male instructor aged 52? 4.012 4.302 4.203 4.159 Now, let's superimpose our parallel regression lines onto the scatterplot of teaching score against age: coeff &lt;- par.model %&gt;% coef() %&gt;% as.numeric() slopes &lt;- eval.score %&gt;% group_by(gender) %&gt;% summarise(min = min(age), max = max(age)) %&gt;% mutate(intercept = coeff[1]) %&gt;% mutate(intercept = ifelse(gender == &quot;male&quot;, intercept + coeff[3], intercept)) %&gt;% gather(point, age, -c(gender, intercept)) %&gt;% #gathers columns into rows #See Data Wrangling Cheat Sheet mutate(y_hat = intercept + age * coeff[2]) ggplot(eval.score, aes(x = age, y = score, col = gender)) + geom_jitter() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_line(data = slopes, aes(y = y_hat), size = 1) Figure 20: Instructor evaluation scores by age and gender with parallel regression lines superimposed. Note: go through the code used to create coeff and slopes and make sure you understand it. From the parallel regression lines model both males and females have the same slope, that is, the associated effect of age on teaching score is the same for both men and women. Hence, for every one year increase in age, there is an associated decrease in teaching score of 0.009. However, male instructors have a higher intercept term, that is, there is a vertical bump in the regression line for males in teaching scores. This is linked to the average difference in teaching scores that males obtain relative to females. What is different between our previous scatterplot of teaching score against age (Figure 19) and the one we just created with our parallel lines superimposed (Figure 20)? In the original plot we have what is referred to as an interaction effect between age and gender. Hence, gender interacts in different ways for both males and females by age, and as such we should have different intercepts and slopes. Multiple regression: interaction model There is an interaction effect if the associated effect of one variable depends on the value of another variable. For example, the effect of age here will depend on whether the instructor is male or female, that is, the effect of age on teaching scores will differ by gender. The interaction model can be written as: \\[\\begin{align} y_{i} &amp;= \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{1i} \\cdot x_{2i} + \\epsilon_i \\nonumber \\\\ &amp;= \\alpha + \\beta_{\\mbox{age}} \\cdot \\mbox{age}_i + \\beta_{\\mbox{male}} \\cdot \\mathbb{I}_{\\mbox{male}}(i) + \\beta_{\\mbox{age, male}} \\cdot \\mbox{age}_i \\cdot \\mathbb{I}_{\\mbox{male}}(i) + \\epsilon_i, \\nonumber \\end{align}\\] where \\(\\beta_{\\mbox{age, male}} \\cdot \\mbox{age}_i \\cdot \\mathbb{I}_{\\mbox{male}}(i)\\) corresponds to the interaction term. In order to fit an interaction term within our regression model we replace the + sign with the * sign as follows: int.model &lt;- lm(score ~ age * gender, data = eval.score) get_regression_table(int.model) # A tibble: 4 x 7 term estimate std_error statistic p_value lower_ci upper_ci &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 intercept 4.88 0.205 23.8 0 4.48 5.29 2 age -0.018 0.004 -3.92 0 -0.026 -0.009 3 gender: male -0.446 0.265 -1.68 0.094 -0.968 0.076 4 age:gendermale 0.014 0.006 2.45 0.015 0.003 0.024 Hence, the regression line for females is given by: \\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age},\\] while the regression line for males is given by: \\[\\widehat{\\mbox{score}} = 4.88 - 0.018 \\cdot \\mbox{age} - 0.446 + 0.014 \\cdot \\mbox{age} = 4.434 - 0.004 \\cdot \\mbox{age}.\\] Notice how the interaction model allows for different slopes for females and males (-0.018 and -0.004, respectively). These fitted lines correspond to the fitted lines we first saw in Figure 19, repeated in Figure 21 but without the jittering: ggplot(eval.score, aes(x = age, y = score, color = gender)) + geom_point() + labs(x = &quot;Age&quot;, y = &quot;Teaching Score&quot;, color = &quot;Gender&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) Figure 21: Instructor evaluation scores by age and gender with interaction model fit added (same as Figure 19 but without jittering). From the interaction model, what would be the teaching score of a female instructor aged 37? 4.286 4.311 4.214 4.227 From the interaction model, what would be the teaching score of a male instructor aged 52? 4.298 4.226 3.944 3.877 How do they compare with the teaching score values from the parallel regression lines model? Here, we can see that, although the intercept for male instructors may be lower, the associated average decrease in teaching score with every year increase in age (0.004) is not as severe as it is for female instructors (0.018). Assessing model fit Now we have to assess the fit of the model by looking at plots of the residuals. We shall do this for the interaction model. First, we need to obtain the fitted values and residuals from the interaction model as follows: regression.points &lt;- get_regression_points(int.model) # A tibble: 463 x 6 ID score age gender score_hat residual &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 4.7 36 female 4.25 0.448 2 2 4.1 36 female 4.25 -0.152 3 3 3.9 36 female 4.25 -0.352 4 4 4.8 36 female 4.25 0.548 5 5 4.6 59 male 4.20 0.399 6 6 4.3 59 male 4.20 0.099 7 7 2.8 59 male 4.20 -1.40 8 8 4.1 51 male 4.23 -0.133 9 9 3.4 51 male 4.23 -0.833 10 10 4.5 40 female 4.18 0.318 # ... with 453 more rows # i Use `print(n = ...)` to see more rows Let's start by looking at a scatterplot of the residuals against the explanatory variable by gender: ggplot(regression.points, aes(x = age, y = residual)) + geom_point() + labs(x = &quot;age&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) + facet_wrap(~ gender) Figure 22: Residuals vs the explanatory variable age by gender. Do the assumptions of the residuals having mean zero and constant variability across all levels of the explanatory variable hold? Yes No Answer There is an even scatter of points above and below the zero line indicating the residuals have mean zero. The scattering of the points is also constant across all values of the explanatory variable with no systematic pattern observed in the residuals. Now, we can plot the residuals against the fitted values: ggplot(regression.points, aes(x = score_hat, y = residual)) + geom_point() + labs(x = &quot;Fitted values&quot;, y = &quot;Residual&quot;) + geom_hline(yintercept = 0, col = &quot;blue&quot;, size = 1) + facet_wrap(~ gender) Figure 23: Residuals vs the fitted values. Do the assumptions of the residuals having mean zero and constant variability across all levels of the fitted values hold? Yes No Answer There is an even scatter of points above and below the zero line indicating the residuals have mean zero. The scattering of the points is also constant across all values of the fitted values with no systematic pattern observed in the residuals. We can also see that the range of fitted values for male instructors is narrower than that of female instructors. Finally, let's plot histograms of the residuals to assess whether they are normally distributed with mean zero: ggplot(regression.points, aes(x = residual)) + geom_histogram(binwidth = 0.25, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) + facet_wrap(~gender) Figure 24: Histograms of the residuals by gender. Do the residuals appear to be normally distributed with mean zero? Yes No Answer Our subjective impression is that the residuals do not appear to be bell-shaped, but rather left-skewed (and more so for males). More formal analysis of the normality of the residuals should be pursued. "],["further-tasks-multiple-linear-regression.html", "Further Tasks: Multiple Linear Regression", " Further Tasks: Multiple Linear Regression You are encouraged to complete the following tasks by using RMarkdown to produce a single document which summarises all your work, i.e. the original questions, your R code, your comments and reflections, etc. Task Assess the model assumptions for the parallel regression lines model. Do they appear valid? Hint Assess the model assumptions in the same way we did for the interaction model. Task Return to the Credit data set and fit a multiple regression model with Balance as the outcome variable, and Income and Age as the explanatory variables, respectively. Assess the assumptions of the multiple regression model. Hint Start by using the select function to subset the Credit data set so you only have the variables Balance, Income and Age. Remember to check the relationship between Balance and Age using a scatterplot before moving on to the modelling process. Task Return to the Credit data set and fit a parallel regression lines model with Balance as the outcome variable, and Income and Student as the explanatory variables, respectively. Assess the assumptions of the fitted model. Task Load the library datasets and look at the iris data set of Edgar Anderson containing measurements (in centimetres) on 150 different flowers across three different species of iris. Fit an interaction model with Sepal.Width as the outcome variable, and Sepal.Length and Species as the explanatory variables. Assess the assumptions of the fitted model. Hint Start by using the select function to subset the iris data set so you only have the variables Sepal.Width, Sepal.Length and Species. Remember to first check the relationship between the variables using summary statistics and scatterplots before moving on to the modelling process. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
